<!DOCTYPE HTML>
<html>
<head>
  <meta charset="utf-8">
  
  <title>golang-memory-leak-failure | 鱼的记忆</title>

  
  <meta name="author" content="比特鱼">
  

  
  <meta name="description" content="一些速记与思考，避免知识的遗忘">
  

  
  <meta name="keywords" content="Kubernetes, Kubeflow, Golang, Java">
  

  <meta id="viewport" name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, minimum-scale=1, user-scalable=no, minimal-ui">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">

  <meta property="og:title" content="golang-memory-leak-failure"/>

  <meta property="og:site_name" content="鱼的记忆"/>

  
  <meta property="og:image" content="/favicon.ico"/>
  

  <link href="/favicon.ico" rel="icon">
  <link rel="alternate" href="/atom.xml" title="鱼的记忆" type="application/atom+xml">
  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">
<meta name="generator" content="Hexo 7.1.1"></head>


<body>
<div class="blog">
  <div class="content">

    <header>
  <div class="site-branding">
    <h1 class="site-title">
      <a href="/">鱼的记忆</a>
    </h1>
    <p class="site-description"></p>
  </div>
  <nav class="site-navigation">
    <ul>
      
    </ul>
  </nav>
</header>

    <main class="site-main posts-loop">
    <article>

  
    
    <h3 class="article-title"><span>golang-memory-leak-failure</span></h3>
    
  

  <div class="article-top-meta">
    <span class="posted-on">
      <a href="/passages/golang-memory-leak-failure/" rel="bookmark">
        <time class="entry-date published" datetime="2019-06-25T18:03:23.000Z">
          2019-06-26
        </time>
      </a>
    </span>
  </div>


  

  <div class="article-content">
    <div class="entry">
      
        <h1 id="一次不成功的Golang内存溢出排查"><a href="#一次不成功的Golang内存溢出排查" class="headerlink" title="一次不成功的Golang内存溢出排查"></a>一次不成功的Golang内存溢出排查</h1><p>我们的监控服务是以Prometheus加K8S为基础搭建的，运行一段时间后出现了内存溢出的现象，内存RSS占用高达9G，CPU占用60%。以下是整理的排查思路</p>
<h2 id="PProf排查："><a href="#PProf排查：" class="headerlink" title="PProf排查："></a>PProf排查：</h2><p>我们先查看了应用日志，这些日志内容不断重复，有一定规律，但是当时暂时没有发现特别之处。</p>
<p><img src="/images/memleak-log.png" alt="memleak-log"></p>
<p>接下来，针对Go应用的内存泄露问题，想到了通过官方PProf工具收集应用信息。幸好，Prometheus开启了PProf接口，因此我们可以很方便的得到协程与内存信息。具体过程不表，结果如下：</p>
<span id="more"></span>

<p>部分协程信息如下：</p>
<p><img src="/images/memleak-routines.png" alt="memleak-routines"></p>
<p>部分内存信息如下：</p>
<p><img src="/images/memleak-memories.png" alt="memleak-memories"></p>
<h2 id="分析协程"><a href="#分析协程" class="headerlink" title="分析协程"></a>分析协程</h2><p>我们发现协程数量达到了恐怖的30万个，而且其中有10万处在调用K8S的POD API上，另外还有20万分别是SERVICE API和ENDPOINT API没有在图中列出。接着，再查看了容器环境（Prometheus运行在容器网络中）中与K8S API通信的连接个数，很少，只有一个。</p>
<p>我们知道，K8S API基于gRPC协议通信，GRPC又是基于HTTP2协议，支持在一个TCP连接上多路复用多个请求流。因此，只用少数连接就可以同时处理多个请求，这是没问题的。但是现在只有一个连接，这就有点诡异了，一般HTTP2对于最大并发请求数是有限制的，需要客户端与服务端协商，服务端不会允许这么大的单连接并发。</p>
<p>于是，我们顺着协程栈，逐个翻看相关代码，具体细节不表。从代码中了解到Go HTTP2库对最大并发数的默认限制为1000，超出部分将会等待，也就是说，在某种特殊情况下TCP连接上的等待处理请求数量超过了最大限制，导致后续的类似协程都阻塞在条件变量上，并且呈不断增长的趋势。</p>
<p>因此，协程问题变成了两个子问题：</p>
<ol>
<li>什么情况下，会导致待处理请求数超限</li>
<li>不断增长的协程，是如何触发的</li>
</ol>
<p>关于第一个问题，很遗憾没有找到原因，后来也没有复现。</p>
<p>关于第二个问题，从前面的日志里面找到了一丝端倪。通过在代码里搜索日志内容的方式，我们定位到，不断重复的日志的源头来自于：</p>
<p><img src="/images/memleak-code.png" alt="memleak-code"></p>
<p>由于这段代码逻辑被反复执行，因此日志内容在不断重复输出。而更巧的是，这段代码所在函数还有另一部分功能，恰好就是启动新协程，<strong>通过K8S Client获取POD&#x2F;SERVICE&#x2F;ENDPOINT信息！</strong></p>
<p>通过反向追述这段代码的调用源点，发现它会在Prometheus配置文件发生变化的时候被调用。</p>
<p>最后得出部分结论，由于配置文件的频繁更新触发Prometheus的自动加载机制，因此协程数量不断增长。在环境稳定的情况下，配置文件不会频繁变更，因此协程问题得以解决。</p>
<h2 id="分析内存"><a href="#分析内存" class="headerlink" title="分析内存"></a>分析内存</h2><p>我们先照图回顾一下Go应用的内存布局：</p>
<figure class="highlight pascal"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Sys = HeapSys + StackSys + MspanSys + MCacheSys + BuckHashSys + GCSys + OtherSys</span><br></pre></td></tr></table></figure>

<ul>
<li><strong>Sys</strong> 代表了 RSS，也就是从操作系统的角度来看，Go进程（包括内部各个线程）总共占用的系统内存，也是此次关注的重点（9G）。从用途的角度来看，Go内存可以分为：堆，栈，内部数据结构，GC以及其他</li>
</ul>
<p>这里面比较有意思的是HeapSys和StackSys，其他就不介绍了。</p>
<ul>
<li><p><strong>HeapSys</strong> &#x3D;&#x3D; HeapIdle + HeapInuse 代表了作为堆用途的内存空间。堆内存以Span为单元进行管理，从Span中分配内存给对象，分为Idle（没有分配对象）和Inuse（有对象）两种类型。</p>
</li>
<li><p><strong>HeapAlloc</strong> &#x3D;&#x3D; Alloc 代表了堆上的未回收对象的实际占用空间（包括可回收部分）。注意，这和HeapInuse是有区别的。HeapInuse代表了分配对象的Span的大小，而HeapAlloc代表了实际对象的大小，因此可以认为一个是粗统计，一个是细统计，HeapAlloc &lt;&#x3D; HeapInuse</p>
</li>
<li><p><strong>StackSys</strong> 代表协程栈大小，其实是Stack Span的大小，可以和Idle Span互相转换。不同版本Go语言的最小协程栈大小是不同的，这次问题里面的协程栈大小为8K，因此30万协程2.4G，和图中的Stack部分相符，但还不足以解释剩下6G的堆内存占用量。</p>
</li>
</ul>
<p>从图中可以看出，6G堆内存中有接近1G处于Idle，剩下5G左右的实际对象占用量。</p>
<p>注意到堆上分配了4千多万个对象，如果每个对象都120B，几乎就可以解释5G大小的内存开销了。联想到Prometheus需要将近期指标加载到内存中，如果短时间内累计大量指标，确实有可能会导致这样的情况发生。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>这次排查问题，还是有不完善之处。</p>
<p>时间不够，也没有思路完整解决协程问题。</p>
<p>对于指标过多的问题，解决方案只能是减少指标监控数据，治标不治本。</p>

      
    </div>

  </div>

  <div class="article-footer">
    <div class="article-meta pull-left">

    
      

    <span class="post-categories">
      <i class="icon-categories"></i>
        <a href="/categories/技术/">技术</a>, <a href="/categories/技术/Golang/">Golang</a>
    </span>
    

    
    

    <span class="post-tags">
      <i class="icon-tags"></i>
        <a href="/tags/Golang/">Golang</a><a href="/tags/MemoryLeak/">MemoryLeak</a>
    </span>
    

    </div>

    
  </div>
</article>

  









    </main>

    <footer class="site-footer">
  <p class="site-info">
    Proudly powered by <a href="https://hexo.io/" target="_blank">Hexo</a> and
    Theme by <a href="https://github.com/CodeDaraW/Hacker" target="_blank">Hacker</a>
    </br>
    
    &copy; 2024 比特鱼
    
  </p>
</footer>
    
    
  </div>
</div>
</body>
</html>